'average_degree': [
average_degree(G_before_2013),
average_degree(G_2013_2017),
average_degree(G_2018_2021),
average_degree(G_2022_2023)
],
'linear_density': [
linear_density(G_before_2013),
linear_density(G_2013_2017),
linear_density(G_2018_2021),
linear_density(G_2022_2023)
]
})
quit
#| label: evolution-graphs-density-r
library(Hmisc)
# Load the 'density_df' dataframe from Python using reticulate
testtransfer <- py$density_df
# Specify the order of categories for the 'period' column
testtransfer$period <- factor(testtransfer$period, levels = c('before-2013', '2013-2017', '2018-2021', '2022-2023'))
# Use the 'gt()' function to display the dataframe
testtransfer %>%
rename_all(Hmisc::capitalize) %>%
gt() %>%
tab_style(
style = cell_text(weight = "bold", align = "center"),
locations = cells_column_labels()
)
# Create the Plotly graph
fig <- plot_ly(testtransfer, x = ~period, y = ~density, type = 'scatter', mode = 'lines+markers',
text = ~paste("Period=", period, "<br>Density=", density), hoverinfo = "text")
# Show the graph
fig <- fig %>% layout(template = "plotly_white")
fig
#| label: citations-data-preparation
# Load the data of references
list_references <- read_csv2('nlp_references_final_18-08-2023.csv')
# Get the current year
current_year <- as.integer(format(Sys.Date(), "%Y"))
# Perform the following operations on the list_references DataFrame:
# 1. Select the first 32 columns
# 2. Extract the relevant part of the 'citing_art' column
# 3. Rename columns for easier reference
# 4. Reorder the 'scopus_id' column
# 5. Extract the year from the 'prism:coverDate' column
# 6. Calculate the 'citations_per_year' column
# 7. Round the 'citations_per_year' column to two decimal places
# 8. Remove the original 'prism:coverDate' column
list_references <- list_references %>%
select(1:32) %>%
mutate(citing_art = substr(citing_art, 11, nchar(citing_art))) %>%
rename(author = `author-list.author.ce:indexed-name`,
scopus_id = `scopus-id`,
citedby_count = `citedby-count`) %>%
relocate(scopus_id, .after = citing_art) %>%
mutate(year = as.integer(substr(`prism:coverDate`, 1, 4))) %>%
mutate(citations_per_year = ifelse(!is.na(citedby_count) & !is.na(year),
citedby_count / (current_year - year + 1),
NA)) %>%
mutate(citations_per_year = round(citations_per_year, 2)) %>%
mutate(year = as.character(year)) %>%
select(-`prism:coverDate`)
skim(list_references)
reticulate::repl_python()
#| label: citations-detect-inconsistencies
list_references = r.list_references
# Group by 'scopus_id' and count the unique number of 'title' for each 'scopus_id'
title_counts = list_references.groupby('scopus_id')['title'].nunique()
# Find the 'scopus_id' that have more than one associated title
inconsistent_scopus_id = title_counts[title_counts > 1].index.tolist()
quit
#| label: citations-show-inconsistencies
list_inconsistencies <- list_references %>%
filter(scopus_id %in% py$inconsistent_scopus_id) %>% #we take the inconsistent scopus_id from python by using reticulate
select(scopus_id, citing_art, title, sourcetitle, year, author)
reactable(
list_inconsistencies,
striped = TRUE,
groupBy = "scopus_id",
defaultPageSize = 5,
defaultColDef = colDef(minWidth = 100, maxWidth = 200),  # Adjust these values as needed
columns = list(
title = colDef(minWidth = 250)  # Adjust this value based on the length of your titles
)
)
reticulate::repl_python()
#| label: citations-correct-inconsistencies
def standardize_values(df, groupby_column, value_column):
"""
Standardize the values of the specified column based on the most frequent non-empty value and fewest characters
within each group.
Parameters:
- df: DataFrame
- groupby_column: The column by which we group data.
- value_column: The column whose values we want to standardize based on the rules.
Returns:
- DataFrame with standardized values.
"""
def custom_mode(series):
# Remove NA values and other representations of NA
series = series.dropna()
series = series[~series.isin(['', 'NA'])]
# If all values were NA or empty
if series.empty:
return np.nan  # Using numpy's nan for consistency
# Get value counts
counts = series.value_counts()
# If there's a single most common value, return it
if len(counts) == 1 or counts.iloc[0] != counts.iloc[1]:
return counts.idxmax()
# If multiple values have the same max count, apply further rules
top_values = counts[counts == counts.iloc[0]].index.tolist()
# Sort by fewest characters
sorted_by_chars = sorted(top_values, key=lambda x: len(x))
# If there's a single value with the fewest characters, return it
if len(sorted_by_chars) == 1 or len(sorted_by_chars[0]) != len(sorted_by_chars[1]):
return sorted_by_chars[0]
# If the column is not the author's name, apply the uppercase letter rule.
if value_column != "author_name":  # adjust "author_name" to the correct column name if necessary
return sorted(sorted_by_chars, key=lambda x: sum(1 for c in x if c.isupper()), reverse=True)[0]
else:
return sorted_by_chars[0]
# Find the most common value for each group based on the custom mode
most_common_value = df.groupby(groupby_column)[value_column].apply(custom_mode).to_dict()
# Map the most common values to the dataframe based on the group
df[value_column] = df[groupby_column].map(most_common_value)
return df
# Usage example:
list_references_standardized = standardize_values(list_references, 'scopus_id', 'title')
list_references_standardized = standardize_values(list_references_standardized, 'scopus_id', 'sourcetitle')
list_references_standardized = standardize_values(list_references_standardized, 'scopus_id', 'author')
quit
#| label: citations-check-inconsistencies
check_inconsistencies <- py$list_references_standardized %>%
filter(scopus_id %in% py$inconsistent_scopus_id) %>% #we take the inconsistent scopus_id from python by using reticulate
select(scopus_id, citing_art, title, sourcetitle, year, author)
reactable(
check_inconsistencies,
striped = TRUE,
defaultPageSize = 5,
groupBy = "scopus_id",
defaultColDef = colDef(minWidth = 100, maxWidth = 200),
columns = list(
title = colDef(minWidth = 250)
)
)
#| label: citations-get-missing-data-todo
# First, let's construct a df where "year" is missing::
missing_years <- py$list_references_standardized %>%
filter(year == "NA") %>%
select(scopus_id, citing_art, title, sourcetitle, year, author, `ce:doi`)
reticulate::repl_python()
#| label: citations-construct-dataframes
def get_citations_df(df, start_year=None, end_year=None):
"""
Filter and extract necessary columns for citation network from a DataFrame based on a range of years.
Parameters:
- df: DataFrame containing the data
- start_year: Optional, the starting year for filtering
- end_year: Optional, the ending year for filtering
Returns:
- DataFrame with filtered data
"""
# Replace 'NA' with numpy.nan and reassign the DataFrame
df = df.replace({'year': 'NA'}, np.nan)
# Drop rows where 'year' is NaN
df = df.dropna(subset=['year'])
# Convert the 'year' column to integer using .astype
df['year'] = df['year'].astype(int)
# Filter the data based on the 'year' column only if start_year and end_year are provided
if start_year is not None and end_year is not None:
df = df[df['year'].between(start_year, end_year)]
# Extract necessary columns for the citation network
citations_df = df[['citing_art', 'scopus_id', 'sourcetitle', 'title', 'citedby_count', 'citations_per_year' , 'author', 'year']]
return citations_df
# Using the function to get a->b standardized data for the citations networks below
citations_df_2022_2023 = get_citations_df(list_references_standardized, 2022, 2023)
citations_df_2018_2021 = get_citations_df(list_references_standardized, 2018, 2021)
citations_df_2013_2017 = get_citations_df(list_references_standardized, 2013, 2017)
citations_df_before_2013 = get_citations_df(list_references_standardized, 0, 2012)
citations_df_overall = get_citations_df(list_references_standardized)  #No filter on years:
#| label: citations-add-information
def get_info_references_dict(df, key, column):
"""
Create a dictionary with keys from the specified key_column and values from the specified value_column.
:param df: Input DataFrame.
:param key_column: Column name to be used as keys in the resulting dictionary.
:param value_column: Column name to be used as values in the resulting dictionary.
:return: Dictionary with keys from key_column and values from value_column.
"""
if key not in df.columns or column not in df.columns:
raise ValueError("The required columns are not present in the DataFrame.")
return sort_dict(df.set_index(key)[column].to_dict())
# We also need to get the info of the citing articles, otherwise we won't get any info when we click
# on the nodes and we will have the number of the node as node label instead of the author name
# Create the 'citing_art' column by stripping the first 10 characters from 'dc_identifier'
data['citing_art'] = data['dc_identifier'].str[10:]
# Getting the current year
current_year = datetime.now().year
# English Comment: Function to calculate citations per year, handles NaN values, division by zero, and the current year.
def calculate_citations_per_year(row):
if pd.isna(row['year']):
return np.nan
elif (current_year - row['year']) == 0:
return 0  # Handle division by zero by returning 0
else:
return round(row['citedby_count'] / (current_year - row['year']), 2)
# Creating the new column 'citations_per_year'
data['citations_per_year'] = data.apply(calculate_citations_per_year, axis=1)
# List of columns to use in the networks later as attributes. We can add more columns if we want to.
columns_to_extract = ['title', 'sourcetitle', 'citedby_count', 'author', 'year', 'citations_per_year']
# Dictionary of dataframes with their respective names
dfs = {
"2022_2023": citations_df_2022_2023,
"2018_2021": citations_df_2018_2021,
"2013_2017": citations_df_2013_2017,
"before_2013": citations_df_before_2013,
"overall": citations_df_overall
}
# Map old column names to new column names
column_mapping = {
'title': 'dc_title',
'sourcetitle': 'prism_publicationName',
'author': 'dc_creator',
'year': 'year',
'citations': 'citedby_count',
'citations_per_year': 'citations_per_year'
}
# Reverse mapping for merging
reverse_column_mapping = {v: k for k, v in column_mapping.items()}
# Rename columns in data DataFrame for merging
data.rename(columns=reverse_column_mapping, inplace=True)
# Initialize the output dictionary
dict_references = {}
# Retrieve the information for each period and each column
for period, df in dfs.items():
dict_references[period] = {}
for column in columns_to_extract:
# This check is important in case all columns are not present across all dataframes
if column in df.columns:
dict_references[period][column] = get_info_references_dict(df, 'scopus_id', column)
# Get the citing_art dictionary from 'data' DataFrame
for column in columns_to_extract:
if column in data.columns:
citing_art_dict = get_info_references_dict(data, 'citing_art', column)
# Add to dict_references only if key is not already present
# It means that the article in our data has been cited by others and is then already present in the references dataframe
for key, value in citing_art_dict.items():
if key not in dict_references[period].get(column, {}):
dict_references[period].setdefault(column, {})[key] = value
#print(dict_references)
# Just to check if the key is in the dictionary
#if "85149566147" in dict_references['2022_2023']['author']:
#print("The key '85149566147' exists.")
#else:
#print("The key '85149566147' does not exist.")
#| label: citations-construct-networks
def sigma_graph_references(dataframe, period_label):
# Create a graph from the given dataframe
G = nx.from_pandas_edgelist(dataframe, 'citing_art', 'scopus_id', create_using=nx.DiGraph())
# Fetch attributes for the given period from the global dict_references
attributes_dict = dict_references.get(period_label, {})
# Set the attributes from dict_references to the nodes of the graph
for attribute, attribute_dict in attributes_dict.items():
nx.set_node_attributes(G, attribute_dict, name=attribute)
# Set edge colors for visualization
for u, v in G.edges:
G[u][v]["color"] = "#7D7C7C"
# Calculate the degree of each node
node_degree = dict(G.degree)
# Compute multiple centrality metrics for nodes
node_degree_centrality = nx.degree_centrality(G)
node_degree_betweenness = nx.betweenness_centrality(G)
node_degree_closeness = nx.closeness_centrality(G)
node_degree_eigenvector = nx.closeness_centrality(G)
#node_degree_constraint_weighted = nx.constraint(G, weight="value")
node_degree_constraint_unweighted = nx.constraint(G)
# Set node attributes for various metrics
nx.set_node_attributes(G, node_degree_centrality, 'centrality')
nx.set_node_attributes(G, node_degree_betweenness, 'betweenness')
nx.set_node_attributes(G, node_degree_closeness, 'closeness')
nx.set_node_attributes(G, node_degree_eigenvector, 'eigenvector centrality')
#nx.set_node_attributes(G, node_degree_constraint_weighted, 'burt\'s constraint weighted')
nx.set_node_attributes(G, node_degree_constraint_unweighted, 'burt constraint unweighted')
# Set node attributes based on the selected 'year_period'
# Construct the sigma graph and customize visualization
Sigma.write_html(G,
default_edge_type      = "arrow",                                                # Set default edge type
fullscreen             = True,                                                   # Display in fullscreen mode
label_density          = 2,                                                      # Increase this to have more labels appear
label_font             = "Helvetica Neue",                                       # Set label font
max_categorical_colors = 30,                                                     # Max categorical colors for communities
node_border_color_from = 'node',                                                 # Set node border color from 'node' attribute
node_color             = "community",                                            # Set node colors
node_label             = "author",                                               # Set node label from 'author' attribute
node_label_size        = G.in_degree,                                            # Set node label size
node_label_size_range  = (12, 36),                                               # Set node label size range
node_metrics           = {"community": {"name": "louvain", "resolution": 1}},    # Specify node metrics
node_size              = G.in_degree,                                            # Set node size based on the in_degree attribute
node_size_range        = (3, 30),                                                # Set node size range
path                   = f"networks/references/{period_label}_sigma.html",       # Specify the output file path
start_layout           = 10                                                       # Start the layout algorithm automatically and lasts 5 seconds
#node_border_color     = "black",                                                # Set node border color
#edge_color            = "source",                                               # Set edge color from 'source' attribute
)
return G
#| label: citations-construct-network-2022-2023
import ipysigma
G_2022_2023_references = sigma_graph_references(citations_df_2022_2023, "2022_2023")
#burt_constraint_unweighted = nx.get_node_attributes(G_2022_2023_references, 'burt constraint unweighted')
#df_burt_constraint = pd.DataFrame(list(burt_constraint_unweighted.items()), columns=['Node', 'Burt_Constraint_Unweighted'])
#df_sorted = df_burt_constraint.sort_values(by='Burt_Constraint_Unweighted')
#print(df_sorted)
SigmaGrid.write_html(
G_2022_2023_references,
views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
node_size_range=(2, 10),
default_edge_type='curve',
path="networks/references/2022_2023_sigma_grid.html",
)
#| label: citations-construct-network-2018-2021
G_2018_2021_references = sigma_graph_references(citations_df_2018_2021, "2018_2021")
#| label: citations-construct-network-2013-2017
G_2013_2017_references = sigma_graph_references(citations_df_2013_2017, "2013_2017")
#| label: citations-construct-network-before-2013
G_before_2013_references = sigma_graph_references(citations_df_before_2013, "before_2013")
#| label: citations-construct-network-overall
G_overall_references = sigma_graph_references(citations_df_overall, "overall")
#| label: citations-graph-density
#| output: false
# Create a dataframe with the density of each graph
density_df_references = pd.DataFrame({
'period': ['before-2013', '2013-2017', '2018-2021', '2022-2023', 'overall'],
'density': [
nx.density(G_before_2013_references),
nx.density(G_2013_2017_references),
nx.density(G_2018_2021_references),
nx.density(G_2022_2023_references),
nx.density(G_overall_references)
],
'average_degree': [
average_degree(G_before_2013_references),
average_degree(G_2013_2017_references),
average_degree(G_2018_2021_references),
average_degree(G_2022_2023_references),
average_degree(G_overall_references)
],
'linear_density': [
linear_density(G_before_2013_references),
linear_density(G_2013_2017_references),
linear_density(G_2018_2021_references),
linear_density(G_2022_2023_references),
linear_density(G_overall_references)
]
})
quit
#| label: citations-graph-density-comparison
#| fig.cap: Comparison of network densities and average degree of nodes over time
#| column: body-outset
# Density plot
density_plot <- ggplot() +
geom_line(data = py$density_df, aes(x = period, y = density, colour = "Collaboration Density", group=1, text = paste("Period:", period, "<br>Density:", density)), linewidth=1) +
geom_line(data = py$density_df_references %>% filter(period != "overall"), aes(x = period, y = density, colour = "References Density", group=1, text = paste("Period:", period, "<br>Density:", density)), linewidth=1) +
scale_y_continuous(name = "Graphs Density") +
scale_x_discrete(limits = c("before-2013", "2013-2017", "2018-2021", "2022-2023")) +
xlab("Period") +
ggtitle("Comparison of Network Density Over Time") +
theme_minimal()
reticulate::repl_python()
from ipysigma import Sigma, SigmaGrid
SigmaGrid.write_html(
G_2022_2023_references,
views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
node_size_range=(2, 10),
default_edge_type='curve',
path="networks/references/2022_2023_sigma_grid.html",
)
SigmaGrid.write_html(
G_2022_2023_references,
views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
node_size_range=(2, 10),
default_edge_type='curve',
path="networks/references/2022_2023_sigma_grid.html",
)
import ipysigma
SigmaGrid.write_html(
G_2022_2023_references,
views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
node_size_range=(2, 10),
default_edge_type='curve',
path="networks/references/2022_2023_sigma_grid.html",
)
import ipysigma
from ipysigma import Sigma, SigmaGrid
G_2022_2023_references = sigma_graph_references(citations_df_2022_2023, "2022_2023")
SigmaGrid.write_html(
G_2022_2023_references,
views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
node_size_range=(2, 10),
default_edge_type='curve',
path="networks/references/2022_2023_sigma_grid.html",
)
import importlib
importlib.reload(ipysigma)
G_2022_2023_references = sigma_graph_references(citations_df_2022_2023, "2022_2023")
SigmaGrid.write_html(
G_2022_2023_references,
views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
node_size_range=(2, 10),
default_edge_type='curve',
path="networks/references/2022_2023_sigma_grid.html",
)
path="networks/references/2022_2023_sigma_grid.html")
SigmaGrid(G_2022_2023_references,
views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
node_size_range=(2, 10),
default_edge_type='curve',
path="networks/references/2022_2023_sigma_grid.html")
)
SigmaGrid(G_2022_2023_references,
views=[{'node_size': G_2022_2023_references.in_degree}, {'node_size': 'citations_year'}],
node_size_range=(2, 10),
default_edge_type='curve',
#path="networks/references/2022_2023_sigma_grid.html"
)
import importlib
print(ipysigma)
print(importlib)
print(ipysigma)
importlib.reload(ipysigma)
print(ipysigma)
import reticulate
library(reticulate)
quit
library(reticulate)
library(reticulate)
reticulate::repl_python()
from ipysigma import Sigma, SigmaGrid
print(ipysigma)
import ipysigma
print(ipysigma)
#| label: library-map-charente
library(rjson)
library(plotly)
# Installer et charger les bibliothèques
install.packages("plotly")
install.packages("sf")
install.packages("plotly")
install.packages("geojsonio")
library(plotly)
library(sf)
library(geojsonio)
# Étape 1: Lire le fichier GeoJSON
charente_geojson <- geojson_read("data/charente.json", what = "sp")
# Étape 2: Préparer vos données
# Supposons que vous ayez un dataframe nommé 'data_communes' avec les données par commune
# data_communes <- ...
# Étape 3: Fusionner les données spatiales avec vos données
# Assurez-vous que les deux ensembles de données ont une colonne commune pour la fusion
# merged_data <- merge(charente_geojson, data_communes, by = "nom_colonne_commune")
# Étape 4: Créer le graphique Choropleth
fig <- plot_ly() %>%
add_trace(
type = "choropleth",
geojson = charente_geojson,
locations = ~nom_colonne_commune,
z = ~votre_colonne_de_donnees  # Par exemple, la population ou toute autre métrique
)
# Afficher la carte
fig
#| label: library-map-charente
library(rjson)
library(plotly)
# Installer et charger les bibliothèques
install.packages("plotly")
install.packages("sf")
install.packages("geojsonio")
library(plotly)
library(sf)
library(geojsonio)
# Étape 1: Lire le fichier GeoJSON
charente_geojson <- geojson_read("data/charente.json", what = "sp")
# Étape 2: Préparer vos données
# Supposons que vous ayez un dataframe nommé 'data_communes' avec les données par commune
# data_communes <- ...
# Étape 3: Fusionner les données spatiales avec vos données
# Assurez-vous que les deux ensembles de données ont une colonne commune pour la fusion
# merged_data <- merge(charente_geojson, data_communes, by = "nom_colonne_commune")
# Étape 4: Créer le graphique Choropleth
fig <- plot_ly() %>%
add_trace(
type = "choropleth",
geojson = charente_geojson,
locations = ~nom_colonne_commune,
z = ~votre_colonne_de_donnees  # Par exemple, la population ou toute autre métrique
)
# Afficher la carte
fig
install.packages("geojsonio")
install.packages("sf")
install.packages("plotly")
library(shiny); runApp('GitHub/dgfip_hackathon/shiny_ui.R')
runApp('GitHub/dgfip_hackathon/shiny_ui.R')
runApp('GitHub/dgfip_hackathon/shiny_ui.R')
runApp('GitHub/dgfip_hackathon/shiny_ui.R')
runApp('GitHub/dgfip_hackathon/shiny_ui.R')
source("~/GitHub/dgfip_hackathon/shiny_ui.R")
source("~/GitHub/dgfip_hackathon/shiny_ui.R")
runApp('GitHub/dgfip_hackathon/shiny_ui.R')
runApp('GitHub/dgfip_hackathon/shiny_ui.R')
runApp('GitHub/dgfip_hackathon/shiny_ui.R')
runApp('GitHub/dgfip_hackathon/shiny_ui.R')
